[{"content":"现在是2025年的10月27日，我还在杭州接受为期两个月的出差煎熬。 每次在这种艰难的生活节点，我就会回想起两年前，在计划着第一次出国的自己。 一次旅行，最开心的时候也许就是在出发前计划它的时候。\n在2023年的11月份，经历完残酷的保研，我突然想在寒假出国旅游一次。 当这个想法来的很突然，也许是9月份的压抑让我感觉到，好像自己的大学四年好像一直在为保研而努力，而现在是时候休息一下了。 本科的时候，因为疫情，我们连外出旅游的机会都很少。我曾经和朋友们去在暑假去成都玩了一周，但是一落地就发现成都出现了病例。所以我们在落地的当天，甚至都要去社区排队做核酸，真的是很不想回忆起来的体验了。 现如今，疫情结束，保研尘埃落定，是时候去放松一下，体验不同的风土人情。\n于是我立刻联系了朋友们，提前准备好护照，看一看去哪个国家。 我其实一直有几个出国的梦想目的地。 一个是美国西雅图，因为在我初中的时候，dota2的TI就是在西雅图举办，给当时小小的我留下了深刻的印象。 其次就是日本了。 在高中的时候，我被高中同学带入了动漫的坑中。 看过动漫的同学都知道，日本对于动漫爱好者来说，是圣地巡礼的天堂。 所谓圣地巡礼，其实就是去动漫的取景地，感受一下这种跨越次元壁，与动漫人物同处于同一个时空的感觉。 无论是《你的名字》中的楼梯，还是《京吹》里的宇治市，等等场景都可以在日本亲自前往，这对我来说，吸引力实在是太大了。 再加上日本应该是在整个出国旅游的选择里，性价比最高的。来回机票不是很贵，与欧洲美国动辄几千上万，加上十几个小时的超长航班相比，淡季来回机票只需要不到2000，总飞行时长只需要3个小时的日本真的是性价比很高的选择。并且和东南亚相比，日本要安全得多；和新加坡马来西亚相比，又不会显得很无聊。 我几乎是立刻拍定了板子，去日本。 经过了一系列的准备，最终我们在24年的1月19日，登上了前往日本的航班。\n我们的整体旅行计划是阪进东出，也就是落地大阪，然后做新干线前往东京，最后从东京回国。 这是很经典的，一次性逛完日本绝大部分景点的路线了。 虽然整体会显得有点走马观花，但是对于第一次去日本的我们，这绝对是很充实的旅行计划了！ 即使是走马观花，也是骑着赤兔马观上野樱花。\n等到我们真正从关西机场出来的时候，已经是下午四点多了。 好不容易折腾好了交通卡，我们直接踏上了前往市中心的电车。 在这之前，我只在北京坐过轨道交通，也就是北京的地铁。 所以第一次做日本的电车，我还是被“惊艳”到了。\n因为日本其实绝大部分的电车是在地上的，一方面是这样的建设成本很低，一方面是地下应该是没有什么空间。 当然在市中心的部分地下铁都是在地下的，只是说相对国内而言，他们会有更多的电车在地上运行。\n我们这次一共有四个人来日本，我们在酒店和民宿中犹豫了很久，最终选择了民宿。 （后面在和其他来日本玩过的朋友聊天的时候才发现，好像不是所有人都喜欢住民宿） 我们当时选择民宿的原因，其实很朴素，就是因为民宿平摊下来的价格比酒店要便宜很多。 另外民宿大家可以住在一起，晚上可以聊天，而酒店只能分开住，空间也相对狭小。\n其实让我现在选择，我觉得住民宿是更能够感受真实日本风土人情的选择。 从机场一路到民宿，我们其实能够明显感觉到从郊区前往市中心，最后到达平民窟的变化过程。 日本的轨道交通真的很发达，发达到就像是任意门一样。 出了地铁站，迎接你的可能是最繁华的市中心，也可能是四下无人，一片死寂的独栋住宅。 狭窄的街道，一般都是单行道，连四个人并排通过都成了难事。 因为没有人行道，所以大多数时候人是和车并排，共享马路的。 民宅紧挨着街道，一栋一栋和拼好的积木一样搭在一起，没有所谓的采光、隐私可言。 头顶的高压线就像喝醉了酒的上班族一样，随意交织在一起。 虽然我来之前看过很多日本的看房视频，但是亲自来了之后还是被这种闭塞给震撼到了。\n到民宿的时候，已经是晚上了。 特种兵旅游是没有休息的，所以我们放下行李直奔心斋桥，顺便找饭吃。\n但是第一次来日本，这个地铁的复杂程度确实是把我们搞晕了。 即使是今年，我再一次去大阪，一上来还是没有搞明白在难波的一系列地铁线路是怎么换乘的。 我们晕晕乎乎坐到了站，我没记错的话应该是心斋桥。 好不容易出了地铁站，\nTo be continued\u0026hellip;\n","date":"2025-10-27T00:00:00Z","image":"https://example.com/post/%E8%AE%B0%E6%97%A5%E6%9C%AC%E6%B8%B8-%E5%85%B6%E4%B8%80/fengmian_hu_402a194bf20bf175.jpg","permalink":"https://example.com/post/%E8%AE%B0%E6%97%A5%E6%9C%AC%E6%B8%B8-%E5%85%B6%E4%B8%80/","title":"记日本游-其一"},{"content":"Skysense Skysense简单来说，是一个能够处理多模态，在遥感下游任务上表现良好的基模。\n他的核心架构如下： 整体结构较为简单，主要分为以下几个部分：\n因子化多模态时空编码器 说的很玄乎，其实就是编码前对向量的处理\n主要做法是，分别利用3个模态的encoder对可见光，多光谱和SAR图像进行编码，然后concat起来，加上时间信息向量（和传统的位置向量有点相似），在头部concat一个cls向量，然后送入encoder得到最终包含时空信息的编码 $F^{mm}_{fus}$。\n得到这个 $F^{mm}_{fus}$之后，结合在预训练中得到的一个全球地理特征表（图右侧），进行一个QKV计算，Q是 $F^{mm}_{fus}$，KV是特征表，这样进行与地理特征表中的特征有关的向量生成，再和 $F^{mm}_{fus}$ concat得到最后的 $F_{fus}$。\n预训练时的设计 首先他们采用的是student-teacher的架构，来最大程度避免RSI的负样本问题\n多粒度对比学习\n这个多粒度就是像素、目标、图像这三个粒度\n分别计算Loss之后加起来\n多模态对齐\n用了一个multi-modal contrastive loss，来把同一位置的不同模态图片信息对齐\n非监督地理信息原型学习\n这个貌似是创新点。\n就是对地球进行了建模，以经度纬度为单位，在小格子里建立多层信息，然后利用$F^{mm}_{fus}$这个语义丰富的向量来训练这个格子，最终让格子里的每一层学习到对应的地理信息，用于辅助下游任务。\n实验设置 他们的主要对比对象有如下模型：\n但是在文中没有写清楚选择标准，同时有一些数据是空缺的，怀疑直接从别人的论文里抄的数据。\n这里我就想吐槽一下这些AI的文章。他们的实验设置讲的都是稀烂。没有工具的选择标准，没有一个对比的基线。\n基本上，每一个任务里的对比模型都不尽相同，而文章中也没有解释，为什么在前一节能够对比的模型，到了这一节却突然消失了。我大发慈悲地猜测，可能是因为每个模型能做到的任务不同，所以可能对于分类任务能够对比，但是分割任务不能对比。而这种信息不应该让读者去猜，而是应该直接写出来。\n这要是高教授来审稿，这里的实验部分肯定是要重写的。\n","date":"2025-10-27T00:00:00Z","image":"https://example.com/post/skysense%E7%AC%94%E8%AE%B0/skysense_hu_fddd7697cc316886.png","permalink":"https://example.com/post/skysense%E7%AC%94%E8%AE%B0/","title":"Skysense笔记"},{"content":"Transformer学习笔记 本篇笔记记录了笔者在学习transformer架构时的一些心得体会。笔者的学习资料来自科技猛兽的博客。也请大家去阅读原文，原文的介绍更加详细。本文在原文的理解基础上，加入了更详细的代码解读。\ntransformer整体结构的核心点有以下几点：\nSelf-Attention机制 多头注意力 位置编码 Self-Attention 注意力是transformer的核心，他的目标是对输入序列进行全局关注，并最终能够并行计算。\nRNN只顺序计算，并且只能考虑之前序列的输入，不能考虑之后的。 CNN只能考虑一个范围内的输入，内容更加有限\nAttention主要由三个部分计算得到：Q(query), K(key), V(value)\n就像从字典里查询单词的过程一样，对于一个Q，我们取寻找和他相似的K，然后获取K对应的结果V。\n整体的流程如下图：\n对于一个输入序列$I = (a^1, a^2, ..., a^n) = W(x^1, x^2, ..., x^n)$, 其中$x^i$是输入的词向量，$W$是词向量编码矩阵，$a^i$是输入序列的权重向量。\nW的代码表示：\n1 2 # encoder self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx) 其中n_src_vocab是源语言的词表大小，d_word_vec是词向量的维度，pad_idx是填充标签\u0026lt;pad\u0026gt;的索引。对于decoder来说就是n_trg_vocab和d_word_vec。如果源语言和目标语言的编码相似，那么就可以共享参数。\n$I$经过三个对应的转化矩阵，将$a^i$转化为$q^i, k^i, v^i$： $$ q^i = W^qa^i\\\\ k^i = W^ka^i\\\\ v^i = W^va^i $$ 现在我们可以做$q$和$k$的attention，具体来说就是计算点积： $$ Scaled\\ Dot{-}Product\\ Attetion: \\alpha_{i,j} = \\frac{q^i \\cdot k^j}{\\sqrt{d_k}} $$其中$d_k$是$k$和$q$的维度，他们是相等的。这么做有一个归一化的效果。\n接着我们将所有的$\\alpha$进行softmax：\n$$ \\hat{\\alpha_{i,j}} = \\frac{\\exp(\\alpha_{i,j})}{\\sum_j \\exp(\\alpha_{i,j})} $$紧接着，我们使用$\\hat{\\alpha}$和$v$对应相乘来得到$b$,作为最终的输出：\n$$ b^i = \\sum_j \\hat{\\alpha_{i,j}}v^j $$代码实现 在代码中，计算注意力和输出$B$是使用单独模块实现的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class ScaledDotProductAttention(nn.Module): def __init__(self, temperature, attn_dropout=0.1): super().__init__() self.temperature = temperature # 这里就是d_k self.dropout = nn.Dropout(attn_dropout) def forward(self, q, k, v, mask=None): attn = torch.matmul(q / self.temperature, k.transpose(2, 3)) # 此处为Q·K^T/sqrt(d_k) if mask is not None: attn = attn.masked_fill(mask == 0, -1e9) # 如果有，加入超大负数作为掩码 attn = self.dropout(F.softmax(attn, dim=-1)) # softmax操作 output = torch.matmul(attn, v) # 和所有的v相乘计算得到b return output, attn 对QKV的计算在多头自注意力模块实现。\n多头自注意力 所谓多头自注意力，实际上就是将原本的$Q$,$K$,$V$分别拆分成$n\\_head$个子矩阵，然后进行$n\\_head$次注意力计算，最后将所有的结果进行拼接，得到最终的输出。\n对于一个完整的转化矩阵$W^q$，通过变化我们将它拆分为$W^{q,1}, W^{q,2}, ..., W^{q,n\\_head}$,在得到$q^i$之后拆分为对应的$q^{i,1}, q^{i,2}, ..., q^{i,n\\_head}$，与对应的$k$和$v$重新完成attention计算得到对应的$b^{i,1}, b^{i,2}, ..., b^{i,n\\_head}$，最后将他们concat起来，通过一个矩阵调整维度变为最后的$b^i$\n代码实现 在代码实现里，这部分比较巧妙。 为了便于计算，代码中只有一个大的矩阵（以计算$Q$为例）$W^q$，他被定义为(d_model, n_head*d_k)。而对于输入的$A$，在代码中定义为(batch_size, seq_len, d_model)，通过矩阵计算之后，会得到一个$Q$矩阵(batch_size, seq_len, n_head*d_k)。这个$Q$矩阵包含了所有头的$Q$向量，这样通过一次计算我们便得到了所有的头的结果。\n实际上，这个大的矩阵$W^q$我们可以看作是许多头的矩阵的concat: $$W^q = Concat([W^{q,1},W^{q,2},...,W^{q,n\\_head}])$$而我们仅通过简单的reshape操作，就可以将每一个头的$q^i$矩阵从$Q$里拆分出来，送入后续的计算注意力环节。对于$K$和$V$向量同理。\n具体代码实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class MultiHeadAttention(nn.Module): def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1): super().__init__() self.n_head = n_head self.d_k = d_k self.d_v = d_v # 因为是多头，所以对于W^q,W^k,W^v，需要将维度扩展到n_head倍 self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False) self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False) self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False) self.fc = nn.Linear(n_head * d_v, d_model, bias=False) self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5) self.dropout = nn.Dropout(dropout) self.layer_norm = nn.LayerNorm(d_model, eps=1e-6) def forward(self, q, k, v, mask=None): # 此处的q,k,v的输入是enc_input, 也就是文章中提到的a,并未真正的q,k,v,此处的qkv大小是(batch_size, seq_len, d_model) d_k, d_v, n_head = self.d_k, self.d_v, self.n_head sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1) residual = q # Pass through the pre-attention projection: b x lq x (n*dv) # Separate different heads: b x lq x n x dv # 核心部分，一次计算得到所有头的QKV,然后分离为多个头 q = self.w_qs(q).view(sz_b, len_q, n_head, d_k) k = self.w_ks(k).view(sz_b, len_k, n_head, d_k) v = self.w_vs(v).view(sz_b, len_v, n_head, d_v) # Transpose for attention dot product: b x n x lq x dv q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2) if mask is not None: mask = mask.unsqueeze(1) q, attn = self.attention(q, k, v, mask=mask) # Transpose to move the head dimension back: b x lq x n x dv # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv) # 因为计算attention需要变一下形状，现在计算完了原样变回来 q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1) #q (sz_b,len_q,n_head,N * d_k) q = self.dropout(self.fc(q)) q += residual q = self.layer_norm(q) return q, attn 至此，通过多头注意力机制和点积注意力计算，我们已经完成了transformer里核心的注意力模块的实现。\n位置编码 因为是注意力机制，对于同样序列的排列组合，如果没有注意力，模型无法区分。所以需要加上位置编码能够让模型对“我爱你”和“你爱我”进行区分。\n在transformer里，位置编码是认为给定的一个向量$e^i$，这个向量在每一个位置都是唯一且不同的。他会与$a^i$进行相加，从而实现位置编码。\n这里的相加不会损失位置信息。可以按照科技猛兽博主里的思路来理解：\n按照我们正常的理解，应该给单词$x^i$在后面concat一个位置编码$p^i$来表示他的位置。这样的话，当我们在计算单词的词向量的时候，就会有：\n$$ W \\cdot x^i_p = [W^I, W^P] \\cdot \\begin{bmatrix} x^i \\\\ p^i \\end{bmatrix} = W^I \\cdot x^i + W^P \\cdot p^i = a^i + e^i $$所以我们的相加操作在宏观上就是对单词进行了一个位置编码的concat。\n在实际设计中，transformer利用了三角函数来计算位置编码，用$PE$表示。\n$$ PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}}) \\\\ PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}}) $$这里的这个$i$有点抽象，他的取值范围是$[0, d_{model} / 2)$。个人理解，这个$2i$和$2i+1$是为了区分sin和cos的。即对于偶数的隐藏层都是sin,对于奇数的隐藏层都是cos。我在下面换一个写法可能就比较能看清楚了。$pos$则表示单词的位置，对于特定的位置$pos$，都可以计算出包含全部$i$的一个位置编码。即：\n$$ PE(pos) = \\sum_{i=0}^{d_{model}/2} PE_{(pos, 2i)}+PE_{(pos, 2i+1)} $$这个函数有一个很重要的性质就是他的可以线性变化的。即对于一个$PE(pos+k)$，是可以被$PE(pos)$所线性表示的。，这样对于模型来说，它可以学习到相对的位置信息。\n代码实现 在transformer的源码中，对于位置编码的计算，是通过先计算sin和cos函数内部的那个$pos / 10000^{2i/d_{model}}$，然后奇数位置取sin，偶数位置取cos。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class PositionalEncoding(nn.Module): def __init__(self, d_hid, n_position=200): super().__init__() # Not a parameter self.register_buffer(\u0026#39;pos_table\u0026#39;, self._get_sinusoid_encoding_table(n_position, d_hid)) def _get_sinusoid_encoding_table(self, n_position, d_hid): def get_position_angle_vec(position): # 对所有隐藏层两两编码，使2i和2i+1相等。 return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)] sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)]) # 对2i使用sin，对2i+1使用cos sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) return torch.FloatTensor(sinusoid_table).unsqueeze(0) def forward(self, x): return x + self.pos_table[:, :x.size(1)].clone().detach() Transformer整体拼装 我们完成了对于transformer里的核心部分的梳理，接下来就是根据论文结果，对应实现encoder和decoder，将所有模块拼装到一起。\nencoder层 整体encoder层的结构如下：\n让我们从下往上一点一点开始。\nInputEmbedding 首先是InputEmbedding，这块使用的是一个nn.Embedding层实现的：\n1 self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx) 这里的n_src_vocab是源语言的词表大小，d_word_vec是词向量的维度，pad_idx是填充的索引。\nPositionalEncoding 然后是我们介绍过的PositionalEncoding层，实现已经在前面介绍过了。\nEncoderLayer 接下来是就是多个EncoderLayer层组成的模块。 每一个EncoderLayer层里都有一个MultiHeadAttention层和一个PositionwiseFeedForward层，并且使用残差和层归一化连接。\nMultiHeadAttention层在前面介绍过了，这里不再赘述。\nPositionwiseFeedForward层实际上就是一个两层全连接层，并且使用ReLU激活函数。他的实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class PositionwiseFeedForward(nn.Module): def __init__(self, d_in, d_hid, dropout=0.1): super().__init__() self.w_1 = nn.Linear(d_in, d_hid) self.w_2 = nn.Linear(d_hid, d_in) self.layer_norm = nn.LayerNorm(d_in, eps=1e-6) self.dropout = nn.Dropout(dropout) def forward(self, x): residual = x # Linear -\u0026gt; ReLU -\u0026gt; Linear x = self.w_2(F.relu(self.w_1(x))) x = self.dropout(x) x += residual x = self.layer_norm(x) return x encoder整体回顾 综上所述，整体的encoder层代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Encoder(nn.Module): def __init__( self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v, d_model, d_inner, pad_idx, dropout=0.1, n_position=200): super().__init__() self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx) self.position_env = PositionalEncoding(d_word_vec, n_position) self.dropout = nn.Dropout(dropout) self.layer_stack = nn.ModuleList([ EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout) for _ in range(n_layers) ]) self.layer_norm = nn.LayerNorm(d_model, eps=1e-6) def forward(self, src_seq, src_mask, return_attns=False): enc_slf_attn_list = [] # encoderLayer前的预处理 enc_output = self.dropout(self.position_env(self.src_word_emb(src_seq))) enc_output = self.layer_norm(enc_output) # encoderLayer for enc_layer in self.layer_stack: enc_output, enc_slf_attn = enc_layer( enc_output, slf_attn_mask=src_mask) enc_slf_attn_list += [enc_slf_attn] if return_attns else [] if return_attns: return enc_output, enc_slf_attn_list return enc_output 可以看到整体encoder的结构还是非常清晰的。最终获得了所有encoderLayer的输出，并返回，用于输入给decoder。\ndecoder层 整个decoder层的结构如下：\n与encoder类似，decoder的下端输入也是一个embedding向量trg_word_emb加上位置编码。只不过此时的embedding是目标语言的，而encoder的是源语言的。\n我们重点来看一下整体的DecoderLayer类。\nDecoderLayer 与EncoderLayer类不同，DecoderLayer面对不同的层，他的输入是不同的。\n在第一层多头掩码注意力层中，输入是trg_word_emb加上位置编码的dec_input。\n1 dec_output, dec_slf_attn = self.slf_attn(dec_input, dec_input, dec_input, mask=slf_attn_mask) 接下来，第二个多头注意力层的K和V是encoder层的最终输出，而Q是刚才得到的dec_output。\n1 2 dec_output, dec_enc_attn = self.enc_attn( dec_output, enc_output, enc_output, mask=dec_enc_attn_mask) 这一步其实就是在匹配目标语言和输入语言。\n最后接一个前文提到的全连接网络PositionwiseFeedForward，便完成了一层decoder的全过程。\ndecoder整体结构 同理，给出整体的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Decoder(nn.Module): def __init__( self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v, d_model, d_inner, pad_idx, n_position=200, dropout=0.1, scale_emb=False): super().__init__() self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx) self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position) self.dropout = nn.Dropout(dropout) self.layer_stack = nn.ModuleList([ DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout) for _ in range(n_layers) ]) self.layer_norm = nn.LayerNorm(d_model, eps=1e-6) self.scale_emb = scale_emb self.d_model = d_model def forward(self, trg_seq, trg_mask, enc_output, enc_mask, return_attns=False): def_slf_attn_list, dec_enc_attn_list = [], [] dec_output = self.trg_word_emb(trg_seq) if self.scale_emb: dec_output *= self.d_model ** 0.5 dec_output = self.dropout(self.position_enc(dec_output)) dec_output = self.layer_norm(dec_output) for dec_layer in self.layer_stack: dec_output, dec_slf_attn, dec_enc_attn = dec_layer( dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=enc_mask) def_slf_attn_list += [dec_slf_attn] if return_attns else [] dec_enc_attn_list += [dec_enc_attn] if return_attns else [] if return_attns: return dec_output, def_slf_attn_list, dec_enc_attn_list return dec_output 整体和encoder非常相似，要注意需要输入enc_output和对应的掩码信息。\n","date":"2025-10-27T00:00:00Z","image":"https://example.com/post/transformer%E7%AC%94%E8%AE%B0/transformer_hu_4a35ee63c848dd8.png","permalink":"https://example.com/post/transformer%E7%AC%94%E8%AE%B0/","title":"Transformer笔记"},{"content":"《秽翼的尤斯蒂娅》简评 原链接：知乎回答\n《秽翼的尤斯蒂娅》是本人受到舍友推荐游玩的第一部八月社的作品。关于游戏的介绍，我再此就不赘述了，我简单谈一下个人游玩的一些体验和对他人评价的感想。\n请注意，以下内容含有剧透\n以下观点仅代表个人看法，如果和你的价值观出现冲突，那就是你对！\n各人物支线剧情的问题 在游玩了真结局之后，回味各人物结局，会给我带来一种“苟延残喘”的感觉。小动物在真结局路线，面对天使寥寥几语的诱劝就能对凯伊姆产生如此强烈的占有欲，那么当他得知在其他路线上，凯伊姆（玩家）的选择之后，肯定会毅然决然选择让都市坠落吧（笑）。这样回过头去品味得到的感觉，让真结局显得弥足珍贵呢。\n而最直观的游玩体验，便是我们的主人公凯伊姆桑前后不一致的割裂感。在我看来，凯伊姆在真结局之前，是对自己存在的意义这个问题，没有一个明确的答案的。他可能是对女人温柔的，但他绝对不会是沉浸在温柔乡里的人，他是自由的、同时也是空虚的和无目标的。如同在天空中漂浮的蒲公英，自由且无目的的随风飘拂。\n总体来说，“脏翅膀”的支线和主线的关系，就好比树木的主干和枝桠。但我相信游玩到后期，枝桠的剧情并不能唤起玩家的共鸣。\n有人因为支线剧情的敷衍批评作品，但我认为这不是八月社的本意，而是“脏翅膀”作为一个galgame美少女恋爱游戏来说，不得不有的一部分。作为一个galgame，他应该为玩家提供多个可以攻略的对象，并且提供满足不同XP的玩家的结局。而八月社，很明显是把重点放在了树木的主干之上，想要用更大的篇幅去完善整个故事。\n主人公的心理历程 和广大玩家一样，我在游玩过程中，也有着 “这样就说服了吗” 的违和感，在菲奥奈和艾丽丝篇尤为严重。一方面原因可能因为玩的时候没有一字一句去分析，看的太快；另一方面可能就是剧本和人物的变化没有连接的很好。\n但是就主人公凯伊姆的心理变化，我认为是描写的非常清晰的。随着凯伊姆的“阅历”增加，他的心理也在发生变化。有的人认为这是人物塑造得失败，尤其在高潮部分主人公的反复转变让许多玩家觉得无法理解他在想什么。其实在结尾，制作组给出了解释，那就是从故事开始就一直困扰凯伊姆的那句 “存在的意义” 。\n凯伊姆因为大崩坏落入牢狱，受到牢狱的环境影响，认为自己就是牢狱的人。后来随着故事发展，他的活动范围在不断上升，而他因为 “逃避选择” ，所以自然而然把身边的环境和自己进行了绑定。这就是玩家在游玩中体会到的，“这个人怎么说话越来越像贵族一样” 的感觉，也就是主人公前后立场不一样的体现。\n凯伊姆缺乏对人生的明确目标和自身存在意义的思考，所以受到身边的环境影响，而改变了自己的思考方式。理解了这一点，就很好理解他内心的纠结和最终做出的选择了。虽然说他最终最初的选择在我看来有一点儿女情长，但是你就说选没选嘛）\n反观鲁基乌斯，在我游玩的时候，我感觉这个人就是卫宫切嗣的写照。在这种带有明显的功利主义的价值观的影响下，他做出了最符合人物设定的抉择。虽然这可能让玩家对他产生极大的厌恶，但是我却很难对他产生讨厌，因为他确实是向着自己坚信的存在意义不断前进的。\n关于小动物 越是玩到后期，我对小动物越是不能直视。我可以明确的说，制作组绝对受到了基督教的影响，而小动物就是那个以耶稣基督为原型创造出来的救世主一样的人物。\n因为家庭原因，我对基督教相关也有些许了解。所以当我看到小动物的使命 “就是去受尽苦难” ，当我看到原天使被囚禁在塔中的形态，很难不去联想到耶稣基督为了拯救世人而钉十字架，而 “耶稣受难” 也正是基督教的教义之一。\n有人说结局是俗套的，小动物的这种 “讨好型人格” 很出戏之类，如果带入基督教的色彩去思考的话，那么整个故事的结局可以说是作者对自己想要表达的宗教观点的一个很好的说明。亦或者说，这个结局本该如此，因为在圣经里，耶稣也是这么做的。\n作为圣子出生在马棚，代替罪孽深重的人类，去受十字架之苦，只为拯救世人，是不是和小动物的故事很像？你可能在之前觉得无法理解小动物的想法和行为，觉得她就是傻白甜天然善良，但我想告诉你，这不是制作组按照现在的天然女主作为模板设定出来的，而是按照耶稣的原型设计的角色，你会不会觉得对小动物的性格有了新的看法呢?\n总结 《秽翼的尤斯蒂娅》可以说是制作组对基督教的一个很好的宣传或者说致敬。对于各位玩家来说，如果大家能够在游玩完游戏之后，愿意去了解基督教相关故事（了解即可，没有别的意思），它是一部能为你带来很多遐想的作品。而我本人游玩之后，也是希望其他玩家能够带着基督教的背景设定去重新思考故事，可能会给你带来不一样的体验。\n","date":"2025-10-26T00:00:00Z","image":"https://example.com/post/%E7%A7%BD%E7%BF%BC%E7%9A%84%E5%B0%A4%E6%96%AF%E8%92%82%E5%A8%85%E7%AE%80%E8%AF%84/home_hu_ad305993fec513e5.png","permalink":"https://example.com/post/%E7%A7%BD%E7%BF%BC%E7%9A%84%E5%B0%A4%E6%96%AF%E8%92%82%E5%A8%85%E7%AE%80%E8%AF%84/","title":"《秽翼的尤斯蒂娅》简评"},{"content":"终于配好了 终于把这个傻逼stack主题配好了，真是傻逼，真是傻逼。\n改天了解一下hugo ","date":"2025-10-26T00:00:00Z","permalink":"https://example.com/post/hugo-stack%E9%85%8D%E7%BD%AE%E8%B8%A9%E5%9D%91/","title":"Hugo+stack配置踩坑"}]